<p>Hello, in this tutorial I will demonstrate how to generate text from text using
Markov Chains and NGrams.</p>

<!--more-->

<p>If you does not know what a Markov Chain or a NGram is, I will demonstrate. At
the end of this tutorial we will implement this text generation tool in Python
without using any library, from the absolute stratch.</p>

<h1 id="what-is-a-markov-chain">What is a Markov Chain?</h1>

<p>Basically, a Markov Chain is something that will take a sequence of “things”, 
these “things” can be anything that can anything that we can represent with
numbers. And, in computers, everything are numbers.</p>

<p>It takes this sequence of numbers and makes some computations to make a new
sequence of numbers, that is random, but still similiar to the original
sequence. I will show a example now.</p>

<p>We have a sequence of As, Bs and Cs: <code class="language-plaintext highlighter-rouge">ABCAABAACACBB</code></p>

<p>First, think the chain as being the following:</p>

<p><img src="/assets/markov-chain-eskeleton.png" alt="Markov Chain Eskeleton" /></p>

<p>We need to calculate the number of times that after A comes B, after A comes A…</p>

<p>Lets make a table:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    A   B   C
A   0   0   0
B   0   0   0
C   0   0   0
</code></pre></div></div>

<p>The sequence is <code class="language-plaintext highlighter-rouge">ABCAABAACACBB</code>, at the start of it, we cal see that after <code class="language-plaintext highlighter-rouge">A</code>
there is a <code class="language-plaintext highlighter-rouge">B</code>. We can increment <code class="language-plaintext highlighter-rouge">A-&gt;B</code>, now the table is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    A   B   C
A   0   1   0
B   0   0   0
C   0   0   0
</code></pre></div></div>

<p>And now <code class="language-plaintext highlighter-rouge">B-&gt;C</code>, <code class="language-plaintext highlighter-rouge">C-&gt;A</code>… The resulting table is the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    A   B   C
A   2   2   2
B   1   1   1
C   2   1   0
</code></pre></div></div>

<p>Now we need to calculate the number of <code class="language-plaintext highlighter-rouge">A-&gt;*</code>, <code class="language-plaintext highlighter-rouge">B-&gt;*</code> and <code class="language-plaintext highlighter-rouge">C-&gt;*</code>, by suming
the rows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    A   B   C   *
A   2   2   2   6
B   1   1   1   3
C   2   1   0   4
</code></pre></div></div>

<p>Now, we will take every item and divide it by the sum:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    A   B   C
A   2/6 2/6 2/6
B   1/3 1/3 1/3
C   2/4 1/4 0/4
</code></pre></div></div>

<p>The resulting table is the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    A   B   C
A   0.3 0.3 0.3
B   0.3 0.3 0.3
C   0.5 0.2 0.0
</code></pre></div></div>

<p>I call it as <code class="language-plaintext highlighter-rouge">probabilites table</code> because it shows the probabilities of
<code class="language-plaintext highlighter-rouge">A-&gt;B</code>, <code class="language-plaintext highlighter-rouge">B-&gt;A</code>…</p>

<p>Now the alst step: create a new sequence that looks similar to the
original one but it is not equal.</p>

<p>For doing it, we will first select a initial state, I will choose <code class="language-plaintext highlighter-rouge">A</code></p>

<p>Now we need to choose the next state, how? Simple, take the row of the
last character:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A   0.3 0.3 0.3
</code></pre></div></div>

<p>Now we take do what I call <code class="language-plaintext highlighter-rouge">weighted random</code>, in the case of <code class="language-plaintext highlighter-rouge">A</code>, all
the possibilites have equals probabilities. Let’s throw a dice, my magical
dice says that the next one is <code class="language-plaintext highlighter-rouge">B</code>.</p>

<p>Our sequence now is <code class="language-plaintext highlighter-rouge">AB</code>, now, lets make the same with <code class="language-plaintext highlighter-rouge">B</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>B   0.3 0.3 0.3
</code></pre></div></div>

<p>The random choose one is <code class="language-plaintext highlighter-rouge">C</code></p>

<p><code class="language-plaintext highlighter-rouge">ABC</code></p>

<p>now with <code class="language-plaintext highlighter-rouge">C</code>, the things are a bit diferent:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>C   0.5 0.2 0.0
</code></pre></div></div>

<p>There is more probability of getting <code class="language-plaintext highlighter-rouge">A</code> than getting <code class="language-plaintext highlighter-rouge">B</code>, and it is
impossible to get <code class="language-plaintext highlighter-rouge">C</code> again.</p>

<p>After keeping doing this for a while, you will have a sequence that
looks like the original one, but a bit different, like differents
executions of a generative art. If you can not perceive that it looks
like the original one, this is normal, humans usually can not find
patterns in this type of data, but there is something where we are
very good at: Text.</p>

<p>Text is our goal, generate text from text. We will use Markov Chains for
doing it, but not a normal Markov Chain, normals Markov Chains does not
works very well for creating text, but there is a alternative: Markov
Chains + NGrams.</p>

<h1 id="what-is-a-ngram">What is a ngram?</h1>

<p>NGrams are ways to divide text in parts, there are different types of
N-Grams: Bigrams, Trigrams…</p>

<p>It may be confusing to undertand, let’s look at a example:</p>

<p>Divide <code class="language-plaintext highlighter-rouge">Hello</code> in N-Grams of 3, or, Trigrams:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Hel
ell
llo
</code></pre></div></div>

<p>Simple, is not it?</p>

<p>Bigrams:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>He
el
ll
lo
</code></pre></div></div>

<h1 id="combining-the-two">Combining the two</h1>

<p>How can we combine NGrams and Markov Chains? It is very simple. Each ngram
has a character after it:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>He -&gt; l
el -&gt; l
ll -&gt; o
</code></pre></div></div>

<p>So, we can make the table:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      'l' 'o'
'He'   1   0
'el'   1   0
'll'   0   1
</code></pre></div></div>

<p>Does it looks simpler now? Well, it is not something so difficult. Let’s create
a new sequence from the ngram <code class="language-plaintext highlighter-rouge">He</code>.</p>

<p>The last bigram of <code class="language-plaintext highlighter-rouge">He</code> is <code class="language-plaintext highlighter-rouge">He</code>, so, weighted random in the row <code class="language-plaintext highlighter-rouge">'He'</code>, it is
<code class="language-plaintext highlighter-rouge">'l'</code>, so, now our sequence is <code class="language-plaintext highlighter-rouge">Hel</code>. The last ngram of <code class="language-plaintext highlighter-rouge">Hel</code> is <code class="language-plaintext highlighter-rouge">el</code>, check
the table, now the sequence is <code class="language-plaintext highlighter-rouge">Hell</code>, last ngram: <code class="language-plaintext highlighter-rouge">ll</code>, <code class="language-plaintext highlighter-rouge">Hello</code>, last ngram is
<code class="language-plaintext highlighter-rouge">lo</code>, but, and now? We never had <code class="language-plaintext highlighter-rouge">lo</code> before, how to continue making text?</p>

<p>My way to contornate it, is now we search for <code class="language-plaintext highlighter-rouge">*o</code> instead of <code class="language-plaintext highlighter-rouge">lo</code>, any ngram
that ends with <code class="language-plaintext highlighter-rouge">o</code>. But we still does not have any gram. In this case, take a
random ngram from the table, my dice says that it is <code class="language-plaintext highlighter-rouge">el</code>, add <code class="language-plaintext highlighter-rouge">el</code> to the
text, we have <code class="language-plaintext highlighter-rouge">Helloel</code>, after <code class="language-plaintext highlighter-rouge">el</code>, it comes <code class="language-plaintext highlighter-rouge">l</code>: <code class="language-plaintext highlighter-rouge">Helloell</code>.</p>

<p>That is it! We generated text, with a higher amount of text we can generate more
complex text. It will never be perfect, because Markov Chains are very weak
compared to RNNs and LSTMs, but sometimes it does the job, this is a example
of text generated with Markov Chain + NGrams:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Data can something.
Data can mean be fa.
Data can be sl, data can be fething, data can nothing, data is data can be sita, data, data can be slData.
Data can be sl nothing.
Data can nothing.
Data can something, data can be cg, data is data can be sl can nothing, data can something, data.
</code></pre></div></div>

<p>This was generated from:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>This is data, data is data.
Data can mean something, data can mean nothing.
Data can be simple, data can be complex.
Computations can be fast, computations can be slow.
</code></pre></div></div>

<p>It is not perfect, but it is not absolute random, you can recognize words,
line breaks that does not break words… It is a bit smart, not 100% random.</p>

<p>If you want to know, it was generated using trigrams, ngrams of 3.</p>

<h1 id="implementing-it">Implementing it</h1>

<p>Now, as I said, we will implement it. I choose Python because it is a simple
language, and so many people know it, so, let’s use it.</p>

